{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **KNN AS A FEATURE ENGINE**\n",
    "\n",
    "KNN as a Feature Engine can aid in ensemble learning by quantifying anecdotal knowledge through supervised machine learning.\n",
    "A good example would be targeting a minority group of customers who are known to have a desirable trait (e.g., similar features/patterns in customer behavior indicative of higher ‘value’ buyers, etc.).\n",
    "- Part one of this two-part series reviews model transformations, metric evaluation, and practices for implementing KNN as a Feature Engine whilst giving users a self-containing script to follow.\n",
    "- Part two of this two-part series incorporates the KNN Feature Engine from part one into different ensemble models and reviews the findings\n",
    "\n",
    "\n",
    "## References\n",
    "- Article Part 1/2: https://analyticalants.co/data-science/knn-as-a-feature-engine-with-imbalanced-data-part-1/\n",
    "- Article Part 2/2: https://analyticalants.co/data-science/knn-as-a-feature-engine-with-imbalanced-data-part-2-2/\n",
    "- Youtube Tutorial: https://youtu.be/C4wwYLum9tE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 1: KNN as a Feature Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "##########################\n",
    "## Establish Environment\n",
    "##########################\n",
    "## Packages\n",
    "  library('pacman')\n",
    "  pacman::p_load(dplyr,rpart,FNN,performanceEstimation,unbalanced, caret, mgcv) \n",
    "## User Defined Parameters\n",
    "  ## KNN Objectives\n",
    "    Model_Evaluation <- 'DEPLOY' ## VALIDATE | TEST | DEPLOY\n",
    "    KNN_TrainSet_Change <- 'ENN' ## NONE | SCALE | SMOTE | TOMEK | ENN \n",
    "  ## KNN Parameters\n",
    "    KNN_K = 19 ##Run once (K=1),then run 'TEST-K' script below for optimization\n",
    "    SMOTE_PrcOver = 3\n",
    "    SMOTE_PrcUnder = 1.34\n",
    "    ENN_K = 3\n",
    "  ## Dataset Parameters\n",
    "    df_Size <- 10000 ## Magnitude of 10 (e.g., 10, 100, 1000, etc.)\n",
    "    df_Imbal <- 0.1 ## Works best <= 0.5\n",
    "    df_Bias <- 'BIAS' ## NONE | BIAS << None = Random, Bias = Bias data\n",
    "  ## Splits\n",
    "    Pc_Train = .80\n",
    "    Pc_valid = .5 ##Allocates this percentage of remaining data to valid\n",
    "## Data\n",
    "  set.seed(2023)\n",
    "  Index <- 1 + (0:(df_Size - 1)) * 1\n",
    "  df<-data.frame(IndexID=as.factor(Index),y=rep(as.factor(c('0','1'))\n",
    "      ,times=c(ceiling(as.integer(df_Size*(1-df_Imbal)))\n",
    "               ,ceiling(as.integer(df_Size*df_Imbal)))))\n",
    "  df$x1=if(df_Bias=='BIAS'){ifelse(df$y=='1',rnorm(sum(df$y=='1'),sd=0.5)\n",
    "                                   ,rnorm(sum(df$y=='0')))}else{rnorm(df_Size)}\n",
    "  df$x2=if(df_Bias=='BIAS'){ifelse(df$y=='1',rnorm(sum(df$y=='1'),sd=0.5)\n",
    "                                   ,rnorm(sum(df$y=='0')))}else{rnorm(df_Size)}\n",
    "## Functions\n",
    "  knn_predict <-function(train_Data, test_Data, train_Class, K_param){\n",
    "    knn(train=train_Data,test=test_Data,cl=train_Class,prob=TRUE,k=K_param)}\n",
    "  knn_evaluate <- function(Type, Knn_Model, TestClass){\n",
    "    cm <- table ( TestClass, Knn_Model )\n",
    "    print ( cm )\n",
    "    if(all(c(0,1) %in% Knn_Model)) {\n",
    "        Accuracy <- round( (cm[1,1]+cm[2,2]) / sum(cm[]), digits = 4) * 100\n",
    "        Precision <- round( cm[2,2] / sum(cm[,2]), digits = 4)* 100\n",
    "        Specificity <- round( cm[1,1] / sum(cm[1,]), digits = 4)* 100\n",
    "        Recall <- round( cm[2,2] / sum(cm[2,]), digits = 4)* 100\n",
    "        F1Score<- round(2*((Precision*Recall)/(Precision+Recall)),digits=2)\n",
    "      print( paste('Model=',Type,': Accuracy=',Accuracy,'% | Precision='\n",
    "                ,Precision,'% | Specificity=',Specificity,'% | Recall=',Recall\n",
    "                ,'% | F1Score=',F1Score,'%'))\n",
    "      }else{print(paste('Model=',Model_Evaluation\n",
    "              ,': Only one class predicted. Metrics cannot be calculated.'))}}\n",
    "##########################\n",
    "## KNN PREP: Split, Scale, SMOTE, TOMEK, ENN\n",
    "##########################\n",
    "## Shuffle & SPLIT\n",
    "  df <- sample_frac(df,1)\n",
    "  tIndex<-createDataPartition(df$y,p=Pc_Train,list=FALSE,times=1)\n",
    "  vIndex<-createDataPartition(df$y[-tIndex],p=Pc_valid,list=FALSE,times=1)\n",
    "    train <- df[tIndex,]\n",
    "    valid <- df[-tIndex,][vIndex,]\n",
    "    test <- df[-tIndex,][-vIndex,]\n",
    "## SCALE\n",
    "  train_scale <- scale ( train[,3:4] )\n",
    "  test_scale <- scale ( test[,3:4] )\n",
    "  valid_scale <- scale ( valid[,3:4] )\n",
    "## SMOTE\n",
    "  train_scale_smote <-\n",
    "    performanceEstimation::smote(y~x1+x2,data=mutate(as.data.frame(train_scale)\n",
    "                ,y=train[,2]),perc.over=SMOTE_PrcOver,perc.under=SMOTE_PrcUnder)\n",
    "## TOMEK\n",
    "  Tomek <- ubTomek( train_scale_smote[,-3], train_scale_smote[,3] )\n",
    "    train_scale_smote_TOMEK <- cbind(Tomek$Y, Tomek$X)\n",
    "    colnames( train_scale_smote_TOMEK) <- c(\"y\", \"x1\" , \"x2\")\n",
    "## ENN\n",
    "  ENN <- ubENN( train_scale_smote[,-3], train_scale_smote[,3] , k = ENN_K )\n",
    "    train_scale_smote_ENN <- cbind(ENN$Y, ENN$X)\n",
    "    colnames( train_scale_smote_ENN) <- c(\"y\", \"x1\" , \"x2\")\n",
    "##########################\n",
    "## KNN\n",
    "##########################\n",
    "## KNN Evaluation Parameters\n",
    "  KNN_Train <- \n",
    "    if (KNN_TrainSet_Change %in% 'NONE') { train[,3:4]\n",
    "      } else if(KNN_TrainSet_Change %in% 'SCALE') {train_scale\n",
    "      } else if(KNN_TrainSet_Change %in% 'SMOTE') {train_scale_smote[,-3]\n",
    "      } else if(KNN_TrainSet_Change %in% 'TOMEK') {train_scale_smote_TOMEK[,-1]\n",
    "      } else if(KNN_TrainSet_Change %in% 'ENN') {train_scale_smote_ENN[,-1]\n",
    "      }\n",
    "  KNN_TrainClass <- \n",
    "    if (KNN_TrainSet_Change %in% c('NONE','SCALE')) {train[,2]\n",
    "      } else if (KNN_TrainSet_Change %in% 'SMOTE') {train_scale_smote[,3]\n",
    "      } else if (KNN_TrainSet_Change %in% 'TOMEK') {train_scale_smote_TOMEK[,1]\n",
    "      } else if (KNN_TrainSet_Change %in% 'ENN') {train_scale_smote_ENN[,1]\n",
    "      }\n",
    "  KNN_Test_Tr<-if(KNN_TrainSet_Change == 'NONE'){train[,3:4]}else{train_scale}\n",
    "  KNN_Test_V<-if(KNN_TrainSet_Change == 'NONE'){test[,3:4]}else{valid_scale}\n",
    "  KNN_Test_Tt<-if(KNN_TrainSet_Change == 'NONE'){valid[,3:4]}else{test_scale}\n",
    "  KNN_TestClass_Tr <- train[,2] \n",
    "  KNN_TestClass_V <- valid[,2] \n",
    "  KNN_TestClass_Tt <-test[,2]\n",
    "## KNN MODELS\n",
    "  KNN_Model_Train <- knn_predict(KNN_Train, KNN_Test_Tr, KNN_TrainClass, KNN_K)\n",
    "  KNN_Model_Valid <- knn_predict(KNN_Train, KNN_Test_V, KNN_TrainClass, KNN_K)\n",
    "  KNN_Model_Test <- knn_predict(KNN_Train, KNN_Test_Tt, KNN_TrainClass, KNN_K)\n",
    "## KNN as Feature Engine (PSEUDO-LABELING|SELF-TRAINING)\n",
    "if(Model_Evaluation=='VALIDATE'){\n",
    "  knn_evaluate('Validate', KNN_Model_Valid, KNN_TestClass_V )} else\n",
    "  if(Model_Evaluation=='TEST')\n",
    "    {knn_evaluate('Test',KNN_Model_Test,KNN_TestClass_Tt)} else\n",
    "  if(Model_Evaluation=='DEPLOY') {\n",
    "  ## Feature Engineering\n",
    "    ## Training set for Ensemble Learning (Part 2)\n",
    "      KNN_Model_Final <- knn_predict(KNN_Train, KNN_Train,KNN_TrainClass, KNN_K)\n",
    "      KNN_Train_Final <- mutate(as.data.frame(KNN_Train),y=KNN_TrainClass)\n",
    "        KNN_Train_Final$Class <- KNN_Model_Final\n",
    "        KNN_Train_Final$ClassScore <- attr( KNN_Model_Final, \"prob\" )\n",
    "        ##View Train Evaluation: \n",
    "          # knn_evaluate('FINAL', KNN_Train_Final[,4], KNN_Train_Final[,3] )\n",
    "    ## Test & Validation sets for Ensemble (Part 2)\n",
    "      KNN_Test_Tr <- mutate(as.data.frame(KNN_Test_Tr),y=KNN_TestClass_Tr)\n",
    "        KNN_Test_Tr$Class <- KNN_Model_Train\n",
    "        KNN_Test_Tr$ClassScore <- attr(KNN_Model_Train, \"prob\")\n",
    "      KNN_Test_V <- mutate(as.data.frame(KNN_Test_V),y=KNN_TestClass_V)\n",
    "        KNN_Test_V$Class <- KNN_Model_Valid\n",
    "        KNN_Test_V$ClassScore <- attr(KNN_Model_Valid, \"prob\")\n",
    "      KNN_Test_Tt <- mutate(as.data.frame(KNN_Test_Tt),y=KNN_TestClass_Tt)\n",
    "        KNN_Test_Tt$Class <- KNN_Model_Test\n",
    "        KNN_Test_Tt$ClassScore <- attr(KNN_Model_Test, \"prob\" )\n",
    "    ## Review Results So Far\n",
    "    df_results <- rbind ( KNN_Test_V, KNN_Test_Tr , KNN_Test_Tt )\n",
    "      knn_evaluate('DEPLOY',df_results[,4],df_results[,3])\n",
    "    ##Clear Memory\n",
    "    rm(df_results) \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 1: TESTING K ( Model_Evaluation == 'VALIDATE'/'TEST')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "k_values <- c(3, 5, 7, 15, 19)  # List of K values to iterate over\n",
    "\n",
    "for (k in k_values) {\n",
    "  KNN_Model_Valid <- knn_predict(KNN_Train, KNN_Test_V, KNN_TrainClass, k)\n",
    "  knn_evaluate(paste('K=', k, sep=''), KNN_Model_Valid, KNN_TestClass_V)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 2: Ensemble Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "##########################\n",
    "## Establish Environment\n",
    "##########################\n",
    "## User Defined Parameters\n",
    "  Model_Evaluation <- 'VALIDATE' ## VALIDATE | TEST | DEPLOY\n",
    "  FeatureEngine_Avail <- 'N' ## Y | N\n",
    "  Tree_y_Weight = .5 ## 0 through 1\n",
    "  Tree_cp = .01\n",
    "  Tree_minsplit = 10\n",
    "## Other Parameters\n",
    "  set.seed(2023)\n",
    "  ## Functions\n",
    "  Model_Predict<-function(model,eval_data,model_name){\n",
    "    predictions <- if(model_name=='Tree'){predict(model,newdata=eval_data)}else\n",
    "      predict(model, newdata = eval_data, type = \"response\")\n",
    "    ifelse(predictions >= .5, 1, 0)\n",
    "  }\n",
    "  Model_Evalaute<-function(model,eval_data_class,model_name){\n",
    "    cm <- if (model_name == 'Tree') {\n",
    "        table ( eval_data_class, model[,2] )\n",
    "        } else table ( eval_data_class, as.vector(model) )\n",
    "    print ( cm )\n",
    "    if(all(c(0,1) %in% model)) {\n",
    "        Accuracy <- round( (cm[1,1]+cm[2,2]) / sum(cm[]), digits = 4) * 100\n",
    "        Precision <- round( cm[2,2] / sum(cm[,2]), digits = 4)* 100\n",
    "        Specificity <- round( cm[1,1] / sum(cm[1,]), digits = 4)* 100\n",
    "        Recall <- round( cm[2,2] / sum(cm[2,]), digits = 4)* 100\n",
    "        F1Score<- round(2*((Precision*Recall)/(Precision+Recall)),digits=2)\n",
    "      print( paste('Model=',model_name,': Accuracy=',Accuracy,'% | Precision='\n",
    "                ,Precision,'% | Specificity=',Specificity,'% | Recall=',Recall\n",
    "                ,'% | F1Score=',F1Score,'%'))\n",
    "      }else{print(paste('Model=',model_name\n",
    "              ,': Only one class predicted. Metrics cannot be calculated.'))}\n",
    "  }\n",
    "  ## Other\n",
    "  if(FeatureEngine_Avail==\"Y\"){\n",
    "        Columns<-c('y','x1','x2','Class')#,'ClassScore')\n",
    "        selected_cols<-c('x1','x2','Class')#,'ClassScore')\n",
    "        Formula=y~x1+x2+Class#+ClassScore\n",
    "      } else if (FeatureEngine_Avail==\"N\"){\n",
    "        Columns<-c('y','x1','x2')\n",
    "        selected_cols<-c('x1','x2')\n",
    "        Formula=y~x1+x2\n",
    "  }\n",
    "##########################\n",
    "## ENSEMBLE PREP\n",
    "##########################\n",
    "  Enmbl_Training <- KNN_Train_Final[Columns]\n",
    "  Enmbl_Vld <- KNN_Test_V[Columns] ## Ensemble Valid\n",
    "  Enmbl_Tst <- KNN_Test_Tt[Columns] ## Ensemble Test\n",
    "  Enmbl_Trn <- KNN_Test_Tr[Columns] ## Ensemble Train\n",
    "  weights = ifelse(Enmbl_Training$y == 1, Tree_y_Weight, (1-Tree_y_Weight))\n",
    "\n",
    "##########################\n",
    "## ENSEMBLE MODELS\n",
    "##########################\n",
    "## Models || GAM note: variable optimization: y ~ s(x)...\n",
    "GAM<-gam(as.formula(Formula),data=Enmbl_Training,family=binomial())\n",
    "GLM<-glm(y~.,data=Enmbl_Training,family=binomial(link=\"logit\"))\n",
    "Tree<-rpart(y~.,data=Enmbl_Training,method=\"class\", weights = weights\n",
    "            ,control = rpart.control(cp=Tree_cp, minsplit = Tree_minsplit)\n",
    "            )\n",
    "## Predictions\n",
    "GAM_Pred_Tr<-Model_Predict(GAM,as.data.frame(Enmbl_Trn[selected_cols]),'GAM')\n",
    "GAM_Pred_V<-Model_Predict(GAM,as.data.frame(Enmbl_Vld[selected_cols]),'GAM')\n",
    "GAM_Pred_Tt<-Model_Predict(GAM,as.data.frame(Enmbl_Tst[selected_cols]),'GAM')\n",
    "GLM_Pred_Tr<-Model_Predict(GLM,as.data.frame(Enmbl_Trn[selected_cols]),'GLM')\n",
    "GLM_Pred_V<-Model_Predict(GLM,as.data.frame(Enmbl_Vld[selected_cols]),'GLM')\n",
    "GLM_Pred_Tt<-Model_Predict(GLM,as.data.frame(Enmbl_Tst[selected_cols]),'GLM')\n",
    "Tree_Pred_Tr<-Model_Predict(Tree,as.data.frame(Enmbl_Trn[selected_cols]),'Tree')\n",
    "Tree_Pred_V<-Model_Predict(Tree,as.data.frame(Enmbl_Vld[selected_cols]),'Tree')\n",
    "Tree_Pred_Tt<-Model_Predict(Tree,as.data.frame(Enmbl_Tst[selected_cols]),'Tree')\n",
    "## Evaluations\n",
    "if (Model_Evaluation == \"VALIDATE\") {\n",
    "    Model_Evalaute(GAM_Pred_V,Enmbl_Vld$y,'GAM' )\n",
    "    Model_Evalaute(GLM_Pred_V,Enmbl_Vld$y,'GLM' )\n",
    "    Model_Evalaute(Tree_Pred_V,Enmbl_Vld$y,'Tree' )\n",
    "  } else if (Model_Evaluation == \"TEST\") {\n",
    "    Model_Evalaute(GAM_Pred_Tt,Enmbl_Tst$y,'GAM' )\n",
    "    Model_Evalaute(GLM_Pred_Tt,Enmbl_Tst$y,'GLM' )\n",
    "    Model_Evalaute(Tree_Pred_Tt,Enmbl_Tst$y,'Tree' )\n",
    "  } else if(Model_Evaluation=='DEPLOY') {\n",
    "    ## Predict Over Train\n",
    "      Enmbl_Trn$GAMClass <- GAM_Pred_Tr\n",
    "      Enmbl_Trn$GLMClass <- GLM_Pred_Tr\n",
    "      Enmbl_Trn$TreeClass <- Tree_Pred_Tr[,2]\n",
    "    ## Predict Over Valid\n",
    "      Enmbl_Vld$GAMClass <- GAM_Pred_V\n",
    "      Enmbl_Vld$GLMClass <- GLM_Pred_V\n",
    "      Enmbl_Vld$TreeClass <- Tree_Pred_V[,2]\n",
    "    ## Predict Over Test\n",
    "      Enmbl_Tst$GAMClass <- GAM_Pred_Tt\n",
    "      Enmbl_Tst$GLMClass <- GLM_Pred_Tt\n",
    "      Enmbl_Tst$TreeClass <- Tree_Pred_Tt[,2]\n",
    "    ## Combine\n",
    "      Ensemble_Final<-rbind(Enmbl_Trn,Enmbl_Vld,Enmbl_Tst)\n",
    "    ## Evaluate >> Code differently based on variables available\n",
    "      if ( FeatureEngine_Avail == 'Y' ) {\n",
    "        Model_Evalaute (Ensemble_Final[,5],Ensemble_Final[,1],'GAM' )\n",
    "        Model_Evalaute (Ensemble_Final[,6],Ensemble_Final[,1],'GLM' )\n",
    "        Model_Evalaute (Ensemble_Final[,7],Ensemble_Final[,1],' Tree' )\n",
    "      } else {\n",
    "        Model_Evalaute (Ensemble_Final[,4],Ensemble_Final[,1],'GAM' )\n",
    "        Model_Evalaute (Ensemble_Final[,5],Ensemble_Final[,1],'GLM' )\n",
    "        Model_Evalaute (Ensemble_Final[,6],Ensemble_Final[,1],' Tree' )\n",
    "      }\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.3.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
